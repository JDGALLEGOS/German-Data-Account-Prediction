{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png)  ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
    "\n",
    "\n",
    "# Predicting Movie Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labversion = 'Taller_big_data-1.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.1 (v3.5.1:37a07cee5969, Dec  6 2015, 01:38:48) [MSC v.1900 32 bit (Intel)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Definir los paths de los archivos\n",
    "#historialPath = \"C:/Users/black/Desktop/Semana_2/Historial.csv\"\n",
    "#historialPaths = \"C:/Users/black/Desktop/Semana_2/Datos/Datos/Tab/Historial.txt\"\n",
    "#antiguedadPaths = \"C:/Users/black/Desktop/Semana_2/Datos/Datos/Tab/Antiguedad.txt\"\n",
    "#orgPersonalPath = \"C:/Users/black/Desktop/Semana_2/Datos/Datos/Tab/Org_personal.txt\"\n",
    "#pensionNominaPath = \"C:/Users/black/Desktop/Semana_2/Datos/Datos/Tab/Pension_nomina.txt\"\n",
    "#personalPath = \"C:/Users/black/Desktop/Semana_2/Datos/Datos/Tab/Personal.txt\"\n",
    "#testPath = \"C:/Users/black/Desktop/Semana_2/Datos/Datos/Tab/test_data.txt\"\n",
    "tablaPath = \"C:/Users/black/Desktop/Semana_2/Datos/Datos/Tab/germanData.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pasar los archivos a un rdd\n",
    "#historial = sc.textFile(historialPath)\n",
    "#historials = sc.textFile(historialPaths)\n",
    "#antiguedad = sc.textFile(antiguedadPaths)\n",
    "#orgPersonal = sc.textFile(orgPersonalPath)\n",
    "#pensionNomina = sc.textFile(pensionNominaPath)\n",
    "#personal = sc.textFile(personalPath)\n",
    "#test = sc.textFile(testPath)\n",
    "tabla = sc.textFile(tablaPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Creditability\\tBalance\\tDuration\\tHistory\\tPurpose\\tCreditAmount\\tSavings\\tEmployment\\tinstPercent\\tSexMarried\\tguarantors\\tResidenceDuration\\tassets\\tage\\tconcCredit\\tapartment\\tcredits\\toccupations\\tdependents\\thasPhone\\tforeign',\n",
       " '1\\t1\\t6\\t4\\t3\\t1169\\t5\\t5\\t4\\t3\\t1\\t4\\t1\\t67\\t3\\t2\\t2\\t3\\t1\\t2\\t1',\n",
       " '0\\t2\\t48\\t2\\t3\\t5951\\t1\\t3\\t2\\t2\\t1\\t2\\t1\\t22\\t3\\t2\\t1\\t3\\t1\\t1\\t1',\n",
       " '1\\t4\\t12\\t4\\t6\\t2096\\t1\\t4\\t2\\t3\\t1\\t3\\t1\\t49\\t3\\t2\\t1\\t2\\t2\\t1\\t1',\n",
       " '1\\t1\\t42\\t2\\t2\\t7882\\t1\\t4\\t2\\t3\\t3\\t4\\t2\\t45\\t3\\t3\\t1\\t3\\t2\\t1\\t1']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabla.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creditability\tBalance\tDuration\tHistory\tPurpose\tCreditAmount\tSavings\tEmployment\tinstPercent\tSexMarried\tguarantors\tResidenceDuration\tassets\tage\tconcCredit\tapartment\tcredits\toccupations\tdependents\thasPhone\tforeign\n"
     ]
    }
   ],
   "source": [
    "# Filter the header row \n",
    "##headerHist = historials.first()\n",
    "#headerAnt = antiguedad.first()\n",
    "#headerOrg = orgPersonal.first()\n",
    "#headerPen = pensionNomina.first()\n",
    "#headerPer = personal.first()\n",
    "#headerTest = test.first()\n",
    "headerTabla = tabla.first()\n",
    "#Print the header of one of the RDDs\n",
    "#headerHist.take(1)\n",
    "print (headerTabla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1\\t1\\t6\\t4\\t3\\t1169\\t5\\t5\\t4\\t3\\t1\\t4\\t1\\t67\\t3\\t2\\t2\\t3\\t1\\t2\\t1',\n",
       " '0\\t2\\t48\\t2\\t3\\t5951\\t1\\t3\\t2\\t2\\t1\\t2\\t1\\t22\\t3\\t2\\t1\\t3\\t1\\t1\\t1',\n",
       " '1\\t4\\t12\\t4\\t6\\t2096\\t1\\t4\\t2\\t3\\t1\\t3\\t1\\t49\\t3\\t2\\t1\\t2\\t2\\t1\\t1',\n",
       " '1\\t1\\t42\\t2\\t2\\t7882\\t1\\t4\\t2\\t3\\t3\\t4\\t2\\t45\\t3\\t3\\t1\\t3\\t2\\t1\\t1',\n",
       " '0\\t1\\t24\\t3\\t0\\t4870\\t1\\t3\\t3\\t3\\t1\\t4\\t4\\t53\\t3\\t3\\t2\\t3\\t2\\t1\\t1']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RDDs without the header\n",
    "#historialNoHeader = historials.filter(lambda x: x!=headerHist)\n",
    "#antiguedadNoHeader = antiguedad.filter(lambda x: x!= headerAnt)\n",
    "#orgPersonalNoHeader = orgPersonal.filter(lambda x: x!= headerOrg)\n",
    "#pensionNominaNoHeader = pensionNomina.filter(lambda x: x!= headerPen)\n",
    "#personalNoHeader = personal.filter(lambda x: x!= headerPer)\n",
    "#testNoHeader = test.filter(lambda x: x!= headerTest)\n",
    "tablaNoHeader = tabla.filter(lambda x: x!= headerTabla)\n",
    "tablaNoHeader.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function to convert string to float\n",
    "def stringToFloat(row):\n",
    "    length = len(row)\n",
    "    counts = []\n",
    "    for t in range(0,length):\n",
    "        counts.append(float(row[t])) \n",
    "\n",
    "    #return [float(row[0]),float(row[1]),float(row[2]),float(row[3])\n",
    "    #         ,float(row[4]),float(row[5]),float(row[6]),float(row[7])\n",
    "    #         ,float(row[8]),float(row[9]),float(row[10]),float(row[11])\n",
    "    #         ,float(row[12]),float(row[13]),float(row[14]),float(row[15])\n",
    "    #         ,float(row[16]),float(row[17]),float(row[18]),float(row[19])\n",
    "    #         ,float(row[20])\n",
    "    #       ]\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0,\n",
       "  1.0,\n",
       "  6.0,\n",
       "  4.0,\n",
       "  3.0,\n",
       "  1169.0,\n",
       "  5.0,\n",
       "  5.0,\n",
       "  4.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  4.0,\n",
       "  1.0,\n",
       "  67.0,\n",
       "  3.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  1.0],\n",
       " [0.0,\n",
       "  2.0,\n",
       "  48.0,\n",
       "  2.0,\n",
       "  3.0,\n",
       "  5951.0,\n",
       "  1.0,\n",
       "  3.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  22.0,\n",
       "  3.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  3.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertir las cadenas de caracteres en rows para que se puedan extraer\n",
    "#historialsRDD = historialNoHeader.map(lambda x:x.split(\"\\t\"))\n",
    "#antiguedadRDD = antiguedadNoHeader.map(lambda x:x.split(\"\\t\"))\n",
    "#orgPersonalRDD = orgPersonalNoHeader.map(lambda x:x.split(\"\\t\"))\n",
    "#PensionNominaRDD = pensionNominaNoHeader.map(lambda x:x.split(\"\\t\"))\n",
    "#personalRDD = personalNoHeader.map(lambda x:x.split(\"\\t\"))\n",
    "#testRDD = testNoHeader.map(lambda x: x.split(\"\\t\")).map(stringToFloat)\n",
    "tablaRDD = tablaNoHeader.map(lambda x: x.split(\"\\t\")).map(stringToFloat)\n",
    "#testRDD2 = testRDD.map(stringToFloat)\n",
    "tablaRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#The functions below parse a line from the data file into the Credit class. \n",
    "#A 1 is subtracted from some categorical values so that they all consistently start with 0.\n",
    "def parseCredit(rdd):\n",
    "    return [rdd[0],rdd[1]-1,rdd[2],rdd[3],rdd[4],rdd[5],rdd[6]-1\n",
    "           ,rdd[7]-1,rdd[8],rdd[9]-1,rdd[10]-1,rdd[11]-1,rdd[12]-1\n",
    "           ,rdd[13],rdd[14]-1,rdd[15]-1,rdd[16]-1,rdd[17]-1\n",
    "           ,rdd[18]-1,rdd[19]-1,rdd[20]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0,\n",
       "  0.0,\n",
       "  6.0,\n",
       "  4.0,\n",
       "  3.0,\n",
       "  1169.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  4.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  3.0,\n",
       "  0.0,\n",
       "  67.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0],\n",
       " [0.0,\n",
       "  1.0,\n",
       "  48.0,\n",
       "  2.0,\n",
       "  3.0,\n",
       "  5951.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  22.0,\n",
       "  2.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  2.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tablaRDD2 =tablaRDD.map(parseCredit)\n",
    "tablaRDD2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2bb9fb31a87a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#from pyspark.sql import HiveContext,Row\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0msqlContext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m#sqlContext = HiveContext(sc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "#importar los paquetes que vamos a usar\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.types import *\n",
    "#from pyspark.sql import HiveContext,Row\n",
    "sqlContext = SQLContext(sc)\n",
    "#sqlContext = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tabla_df_schema = StructType(\n",
    "       [StructField('creditability', DoubleType()),\n",
    "        StructField('balance', DoubleType()),\n",
    "        StructField('duration', DoubleType()),\n",
    "        StructField('history', DoubleType()),\n",
    "        StructField('purpose', DoubleType()),\n",
    "        StructField('amount', DoubleType()),\n",
    "        StructField('savings', DoubleType()),\n",
    "        StructField('employment', DoubleType()),\n",
    "        StructField('instPercent', DoubleType()),\n",
    "        #StructField('sueldo2', StringType()),\n",
    "        StructField('sexMarried', DoubleType()),\n",
    "        StructField('guarantors', DoubleType()),\n",
    "        StructField('residenceDuration', DoubleType()),\n",
    "        StructField('assets', DoubleType()),\n",
    "        StructField('age', DoubleType()),\n",
    "        StructField('concCredit', DoubleType()),\n",
    "        #StructField('activo_ant2', StringType()),\n",
    "        StructField('apartment', DoubleType()),\n",
    "        StructField('credits', DoubleType()),\n",
    "        StructField('occupations', DoubleType()),\n",
    "        StructField('dependents', DoubleType()),\n",
    "        StructField('hasPhone', DoubleType()),\n",
    "        StructField('foreign', DoubleType())\n",
    "       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#powerPlantDF = sqlContext.read.format('com.databricks.spark.csv').options(delimiter=',',header='true',inferschema='true').load('historialPath')\n",
    "#set(powerPlantDF.dtypes)\n",
    "#se crea una tabla con la estructura de arriba y los datos del rdd historialRDD\n",
    "#historialDF = sqlContext.createDataFrame(historialsRDD, schema = historial_df_schema)\n",
    "#antiguedadDF = sqlContext.createDataFrame(antiguedadRDD, schema = antiguedad_df_schema)\n",
    "#orgPersonalDF = sqlContext.createDataFrame(orgPersonalRDD, schema = orgPersonal_df_schema)\n",
    "#penNominaDF = sqlContext.createDataFrame(PensionNominaRDD, schema = penNomina_df_schema)\n",
    "#personalDF = sqlContext.createDataFrame(personalRDD, schema = personal_df_schema)\n",
    "#testDF = sqlContext.createDataFrame(testRDD, schema =test_df_schema)\n",
    "tablaDF = sqlContext.createDataFrame(tablaRDD2, schema =tabla_df_schema).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+--------+-------+-------+-------+-------+----------+-----------+----------+----------+-----------------+------+----+----------+---------+-------+-----------+----------+--------+-------+\n",
      "|creditability|balance|duration|history|purpose| amount|savings|employment|instPercent|sexMarried|guarantors|residenceDuration|assets| age|concCredit|apartment|credits|occupations|dependents|hasPhone|foreign|\n",
      "+-------------+-------+--------+-------+-------+-------+-------+----------+-----------+----------+----------+-----------------+------+----+----------+---------+-------+-----------+----------+--------+-------+\n",
      "|          1.0|    0.0|     6.0|    4.0|    3.0| 1169.0|    4.0|       4.0|        4.0|       2.0|       0.0|              3.0|   0.0|67.0|       2.0|      1.0|    1.0|        2.0|       0.0|     1.0|    0.0|\n",
      "|          0.0|    1.0|    48.0|    2.0|    3.0| 5951.0|    0.0|       2.0|        2.0|       1.0|       0.0|              1.0|   0.0|22.0|       2.0|      1.0|    0.0|        2.0|       0.0|     0.0|    0.0|\n",
      "|          1.0|    3.0|    12.0|    4.0|    6.0| 2096.0|    0.0|       3.0|        2.0|       2.0|       0.0|              2.0|   0.0|49.0|       2.0|      1.0|    0.0|        1.0|       1.0|     0.0|    0.0|\n",
      "|          1.0|    0.0|    42.0|    2.0|    2.0| 7882.0|    0.0|       3.0|        2.0|       2.0|       2.0|              3.0|   1.0|45.0|       2.0|      2.0|    0.0|        2.0|       1.0|     0.0|    0.0|\n",
      "|          0.0|    0.0|    24.0|    3.0|    0.0| 4870.0|    0.0|       2.0|        3.0|       2.0|       0.0|              3.0|   3.0|53.0|       2.0|      2.0|    1.0|        2.0|       1.0|     0.0|    0.0|\n",
      "|          1.0|    3.0|    36.0|    2.0|    6.0| 9055.0|    4.0|       2.0|        2.0|       2.0|       0.0|              3.0|   3.0|35.0|       2.0|      2.0|    0.0|        1.0|       1.0|     1.0|    0.0|\n",
      "|          1.0|    3.0|    24.0|    2.0|    2.0| 2835.0|    2.0|       4.0|        3.0|       2.0|       0.0|              3.0|   1.0|53.0|       2.0|      1.0|    0.0|        2.0|       0.0|     0.0|    0.0|\n",
      "|          1.0|    1.0|    36.0|    2.0|    1.0| 6948.0|    0.0|       2.0|        2.0|       2.0|       0.0|              1.0|   2.0|35.0|       2.0|      0.0|    0.0|        3.0|       0.0|     1.0|    0.0|\n",
      "|          1.0|    3.0|    12.0|    2.0|    3.0| 3059.0|    3.0|       3.0|        2.0|       0.0|       0.0|              3.0|   0.0|61.0|       2.0|      1.0|    0.0|        1.0|       0.0|     0.0|    0.0|\n",
      "|          0.0|    1.0|    30.0|    4.0|    0.0| 5234.0|    0.0|       0.0|        4.0|       3.0|       0.0|              1.0|   2.0|28.0|       2.0|      1.0|    1.0|        3.0|       0.0|     0.0|    0.0|\n",
      "|          0.0|    1.0|    12.0|    2.0|    0.0| 1295.0|    0.0|       1.0|        3.0|       1.0|       0.0|              0.0|   2.0|25.0|       2.0|      0.0|    0.0|        2.0|       0.0|     0.0|    0.0|\n",
      "|          0.0|    0.0|    48.0|    2.0|    9.0| 4308.0|    0.0|       1.0|        3.0|       1.0|       0.0|              3.0|   1.0|24.0|       2.0|      0.0|    0.0|        2.0|       0.0|     0.0|    0.0|\n",
      "|          1.0|    1.0|    12.0|    2.0|    3.0| 1567.0|    0.0|       2.0|        1.0|       1.0|       0.0|              0.0|   2.0|22.0|       2.0|      1.0|    0.0|        2.0|       0.0|     1.0|    0.0|\n",
      "|          0.0|    0.0|    24.0|    4.0|    0.0| 1199.0|    0.0|       4.0|        4.0|       2.0|       0.0|              3.0|   2.0|60.0|       2.0|      1.0|    1.0|        1.0|       0.0|     0.0|    0.0|\n",
      "|          1.0|    0.0|    15.0|    2.0|    0.0| 1403.0|    0.0|       2.0|        2.0|       1.0|       0.0|              3.0|   2.0|28.0|       2.0|      0.0|    0.0|        2.0|       0.0|     0.0|    0.0|\n",
      "|          0.0|    0.0|    24.0|    2.0|    3.0| 1282.0|    1.0|       2.0|        4.0|       1.0|       0.0|              1.0|   2.0|32.0|       2.0|      1.0|    0.0|        1.0|       0.0|     0.0|    0.0|\n",
      "|          1.0|    3.0|    24.0|    4.0|    3.0| 2424.0|    4.0|       4.0|        4.0|       2.0|       0.0|              3.0|   1.0|53.0|       2.0|      1.0|    1.0|        2.0|       0.0|     0.0|    0.0|\n",
      "|          1.0|    0.0|    30.0|    0.0|    9.0| 8072.0|    4.0|       1.0|        2.0|       2.0|       0.0|              2.0|   2.0|25.0|       0.0|      1.0|    2.0|        2.0|       0.0|     0.0|    0.0|\n",
      "|          0.0|    1.0|    24.0|    2.0|    1.0|12579.0|    0.0|       4.0|        4.0|       1.0|       0.0|              1.0|   3.0|44.0|       2.0|      2.0|    0.0|        3.0|       0.0|     1.0|    0.0|\n",
      "|          1.0|    3.0|    24.0|    2.0|    3.0| 3430.0|    2.0|       4.0|        3.0|       2.0|       0.0|              1.0|   2.0|31.0|       2.0|      1.0|    0.0|        2.0|       1.0|     1.0|    0.0|\n",
      "+-------------+-------+--------+-------+-------+-------+-------+----------+-----------+----------+----------+-----------------+------+----+----------+---------+-------+-----------+----------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tablaDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('creditability', 'double'), ('balance', 'double'), ('duration', 'double'), ('history', 'double'), ('purpose', 'double'), ('amount', 'double'), ('savings', 'double'), ('employment', 'double'), ('instPercent', 'double'), ('sexMarried', 'double'), ('guarantors', 'double'), ('residenceDuration', 'double'), ('assets', 'double'), ('age', 'double'), ('concCredit', 'double'), ('apartment', 'double'), ('credits', 'double'), ('occupations', 'double'), ('dependents', 'double'), ('hasPhone', 'double'), ('foreign', 'double')]\n"
     ]
    }
   ],
   "source": [
    "tablaDF.dtypes\n",
    "print (tablaDF.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|summary|     creditability|           balance|          duration|           history|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "|  count|              1000|              1000|              1000|              1000|\n",
      "|   mean|               0.7|             1.577|            20.903|             2.545|\n",
      "| stddev|0.4584868702702513|1.2576377271108932|12.058814452756371|1.0831196370429943|\n",
      "|    min|               0.0|               0.0|               4.0|               0.0|\n",
      "|    max|               1.0|               3.0|              72.0|               4.0|\n",
      "+-------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Computes statistics for numeric columns\n",
    "#tablaDF.describe('viabilidad','aportaciones','sueldo','antiguedad').show()\n",
    "tablaDF.describe('creditability','balance','duration','history').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Register the data.frame as a table. This will allow us to compute the algorithm\n",
    "#sqlContext.sql(\"DROP TABLE IF EXISTS test_data\")\n",
    "sqlContext.registerDataFrameAsTable(tablaDF,\"tabla_data\")\n",
    "#selectDF = tablaDF.select(tablaDF[\"sueldo\"],tablaDF[\"viabilidad\"])\n",
    "#selectDF.show()\n",
    "#sqlContext.dropTempTable(\"test_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[creditability: double, balance: double, duration: double, history: double, purpose: double, amount: double, savings: double, employment: double, instPercent: double, sexMarried: double, guarantors: double, residenceDuration: double, assets: double, age: double, concCredit: double, apartment: double, credits: double, occupations: double, dependents: double, hasPhone: double, foreign: double]>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print out the schema in the tree format\n",
    "tablaDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Data Preparation\n",
    "\n",
    "#The next step is to prepare the data for machine learning. Since all of this data is numeric \n",
    "#and consistent this is a simple and strightforward tark. \n",
    "\n",
    "#The goal is to use machine learning to determine if the scenario is optimal or not. \n",
    "#The first step in building our ML pipeline is to convert the predictor features from Data \n",
    "#Frame columns to Feature Vectors using the pyspark.ml.feature.VectorAssember() method.\n",
    "#The VectorAssember is a transformer that conbines a given list of columns into a single vector columns.\n",
    "#It is useful for combining raw features generated by different feature transformres into a \n",
    "#single feature vector, in order to train ML models like decision trees. VectorAssembler takes a list\n",
    "#of input columns names (each is a string) and the name of the output column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+--------+-------+-------+-------+-------+----------+-----------+----------+----------+-----------------+------+----+----------+---------+-------+-----------+----------+--------+-------+--------------------+\n",
      "|creditability|balance|duration|history|purpose| amount|savings|employment|instPercent|sexMarried|guarantors|residenceDuration|assets| age|concCredit|apartment|credits|occupations|dependents|hasPhone|foreign|            features|\n",
      "+-------------+-------+--------+-------+-------+-------+-------+----------+-----------+----------+----------+-----------------+------+----+----------+---------+-------+-----------+----------+--------+-------+--------------------+\n",
      "|          1.0|    0.0|     6.0|    4.0|    3.0| 1169.0|    4.0|       4.0|        4.0|       2.0|       0.0|              3.0|   0.0|67.0|       2.0|      1.0|    1.0|        2.0|       0.0|     1.0|    0.0|[0.0,6.0,4.0,3.0,...|\n",
      "|          0.0|    1.0|    48.0|    2.0|    3.0| 5951.0|    0.0|       2.0|        2.0|       1.0|       0.0|              1.0|   0.0|22.0|       2.0|      1.0|    0.0|        2.0|       0.0|     0.0|    0.0|[1.0,48.0,2.0,3.0...|\n",
      "|          1.0|    3.0|    12.0|    4.0|    6.0| 2096.0|    0.0|       3.0|        2.0|       2.0|       0.0|              2.0|   0.0|49.0|       2.0|      1.0|    0.0|        1.0|       1.0|     0.0|    0.0|[3.0,12.0,4.0,6.0...|\n",
      "|          1.0|    0.0|    42.0|    2.0|    2.0| 7882.0|    0.0|       3.0|        2.0|       2.0|       2.0|              3.0|   1.0|45.0|       2.0|      2.0|    0.0|        2.0|       1.0|     0.0|    0.0|[0.0,42.0,2.0,2.0...|\n",
      "|          0.0|    0.0|    24.0|    3.0|    0.0| 4870.0|    0.0|       2.0|        3.0|       2.0|       0.0|              3.0|   3.0|53.0|       2.0|      2.0|    1.0|        2.0|       1.0|     0.0|    0.0|[0.0,24.0,3.0,0.0...|\n",
      "|          1.0|    3.0|    36.0|    2.0|    6.0| 9055.0|    4.0|       2.0|        2.0|       2.0|       0.0|              3.0|   3.0|35.0|       2.0|      2.0|    0.0|        1.0|       1.0|     1.0|    0.0|[3.0,36.0,2.0,6.0...|\n",
      "|          1.0|    3.0|    24.0|    2.0|    2.0| 2835.0|    2.0|       4.0|        3.0|       2.0|       0.0|              3.0|   1.0|53.0|       2.0|      1.0|    0.0|        2.0|       0.0|     0.0|    0.0|[3.0,24.0,2.0,2.0...|\n",
      "|          1.0|    1.0|    36.0|    2.0|    1.0| 6948.0|    0.0|       2.0|        2.0|       2.0|       0.0|              1.0|   2.0|35.0|       2.0|      0.0|    0.0|        3.0|       0.0|     1.0|    0.0|[1.0,36.0,2.0,1.0...|\n",
      "|          1.0|    3.0|    12.0|    2.0|    3.0| 3059.0|    3.0|       3.0|        2.0|       0.0|       0.0|              3.0|   0.0|61.0|       2.0|      1.0|    0.0|        1.0|       0.0|     0.0|    0.0|[3.0,12.0,2.0,3.0...|\n",
      "|          0.0|    1.0|    30.0|    4.0|    0.0| 5234.0|    0.0|       0.0|        4.0|       3.0|       0.0|              1.0|   2.0|28.0|       2.0|      1.0|    1.0|        3.0|       0.0|     0.0|    0.0|[1.0,30.0,4.0,0.0...|\n",
      "|          0.0|    1.0|    12.0|    2.0|    0.0| 1295.0|    0.0|       1.0|        3.0|       1.0|       0.0|              0.0|   2.0|25.0|       2.0|      0.0|    0.0|        2.0|       0.0|     0.0|    0.0|(20,[0,1,2,4,6,7,...|\n",
      "|          0.0|    0.0|    48.0|    2.0|    9.0| 4308.0|    0.0|       1.0|        3.0|       1.0|       0.0|              3.0|   1.0|24.0|       2.0|      0.0|    0.0|        2.0|       0.0|     0.0|    0.0|(20,[1,2,3,4,6,7,...|\n",
      "|          1.0|    1.0|    12.0|    2.0|    3.0| 1567.0|    0.0|       2.0|        1.0|       1.0|       0.0|              0.0|   2.0|22.0|       2.0|      1.0|    0.0|        2.0|       0.0|     1.0|    0.0|[1.0,12.0,2.0,3.0...|\n",
      "|          0.0|    0.0|    24.0|    4.0|    0.0| 1199.0|    0.0|       4.0|        4.0|       2.0|       0.0|              3.0|   2.0|60.0|       2.0|      1.0|    1.0|        1.0|       0.0|     0.0|    0.0|[0.0,24.0,4.0,0.0...|\n",
      "|          1.0|    0.0|    15.0|    2.0|    0.0| 1403.0|    0.0|       2.0|        2.0|       1.0|       0.0|              3.0|   2.0|28.0|       2.0|      0.0|    0.0|        2.0|       0.0|     0.0|    0.0|(20,[1,2,4,6,7,8,...|\n",
      "|          0.0|    0.0|    24.0|    2.0|    3.0| 1282.0|    1.0|       2.0|        4.0|       1.0|       0.0|              1.0|   2.0|32.0|       2.0|      1.0|    0.0|        1.0|       0.0|     0.0|    0.0|[0.0,24.0,2.0,3.0...|\n",
      "|          1.0|    3.0|    24.0|    4.0|    3.0| 2424.0|    4.0|       4.0|        4.0|       2.0|       0.0|              3.0|   1.0|53.0|       2.0|      1.0|    1.0|        2.0|       0.0|     0.0|    0.0|[3.0,24.0,4.0,3.0...|\n",
      "|          1.0|    0.0|    30.0|    0.0|    9.0| 8072.0|    4.0|       1.0|        2.0|       2.0|       0.0|              2.0|   2.0|25.0|       0.0|      1.0|    2.0|        2.0|       0.0|     0.0|    0.0|[0.0,30.0,0.0,9.0...|\n",
      "|          0.0|    1.0|    24.0|    2.0|    1.0|12579.0|    0.0|       4.0|        4.0|       1.0|       0.0|              1.0|   3.0|44.0|       2.0|      2.0|    0.0|        3.0|       0.0|     1.0|    0.0|[1.0,24.0,2.0,1.0...|\n",
      "|          1.0|    3.0|    24.0|    2.0|    3.0| 3430.0|    2.0|       4.0|        3.0|       2.0|       0.0|              1.0|   2.0|31.0|       2.0|      1.0|    0.0|        2.0|       1.0|     1.0|    0.0|[3.0,24.0,2.0,3.0...|\n",
      "+-------------+-------+--------+-------+-------+-------+-------+----------+-----------+----------+----------+-----------------+------+----+----------+---------+-------+-----------+----------+--------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#the first step in building our ML pipeline is to convert the predictor features from \n",
    "#DataFrama columns to Feature Vectors using the pyspark.ml.feature.VectorAsember() method. \n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "datasetDF = sqlContext.table(\"tabla_data\")\n",
    "\n",
    "vectorizer = VectorAssembler()\n",
    "vectorizer.setInputCols([\"balance\",\"duration\",\"history\",\"purpose\",\"amount\",\n",
    "                        \"savings\",\"employment\",\"instPercent\",\"sexMarried\",\n",
    "                        \"guarantors\",\"residenceDuration\",\"assets\",\"age\",\"concCredit\",\n",
    "                        \"apartment\",\"credits\",\"occupations\",\"dependents\",\"hasPhone\",\"foreign\"])\n",
    "vectorizer.setOutputCol(\"features\")\n",
    "\n",
    "#return a dataframe with all of the feature columns in a vector columns. \n",
    "#The transform method produced a new column: features.\n",
    "#Transform.- Tranforma los datos de entrada del dataset con parametros optionales. \n",
    "df2 = vectorizer.transform(tablaDF)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+--------+-------+-------+-------+-------+----------+-----------+----------+----------+-----------------+------+----+----------+---------+-------+-----------+----------+--------+-------+--------------------+-----+\n",
      "|creditability|balance|duration|history|purpose| amount|savings|employment|instPercent|sexMarried|guarantors|residenceDuration|assets| age|concCredit|apartment|credits|occupations|dependents|hasPhone|foreign|            features|label|\n",
      "+-------------+-------+--------+-------+-------+-------+-------+----------+-----------+----------+----------+-----------------+------+----+----------+---------+-------+-----------+----------+--------+-------+--------------------+-----+\n",
      "|          1.0|    0.0|     6.0|    4.0|    3.0| 1169.0|    4.0|       4.0|        4.0|       2.0|       0.0|              3.0|   0.0|67.0|       2.0|      1.0|    1.0|        2.0|       0.0|     1.0|    0.0|[0.0,6.0,4.0,3.0,...|  0.0|\n",
      "|          0.0|    1.0|    48.0|    2.0|    3.0| 5951.0|    0.0|       2.0|        2.0|       1.0|       0.0|              1.0|   0.0|22.0|       2.0|      1.0|    0.0|        2.0|       0.0|     0.0|    0.0|[1.0,48.0,2.0,3.0...|  1.0|\n",
      "|          1.0|    3.0|    12.0|    4.0|    6.0| 2096.0|    0.0|       3.0|        2.0|       2.0|       0.0|              2.0|   0.0|49.0|       2.0|      1.0|    0.0|        1.0|       1.0|     0.0|    0.0|[3.0,12.0,4.0,6.0...|  0.0|\n",
      "|          1.0|    0.0|    42.0|    2.0|    2.0| 7882.0|    0.0|       3.0|        2.0|       2.0|       2.0|              3.0|   1.0|45.0|       2.0|      2.0|    0.0|        2.0|       1.0|     0.0|    0.0|[0.0,42.0,2.0,2.0...|  0.0|\n",
      "|          0.0|    0.0|    24.0|    3.0|    0.0| 4870.0|    0.0|       2.0|        3.0|       2.0|       0.0|              3.0|   3.0|53.0|       2.0|      2.0|    1.0|        2.0|       1.0|     0.0|    0.0|[0.0,24.0,3.0,0.0...|  1.0|\n",
      "|          1.0|    3.0|    36.0|    2.0|    6.0| 9055.0|    4.0|       2.0|        2.0|       2.0|       0.0|              3.0|   3.0|35.0|       2.0|      2.0|    0.0|        1.0|       1.0|     1.0|    0.0|[3.0,36.0,2.0,6.0...|  0.0|\n",
      "|          1.0|    3.0|    24.0|    2.0|    2.0| 2835.0|    2.0|       4.0|        3.0|       2.0|       0.0|              3.0|   1.0|53.0|       2.0|      1.0|    0.0|        2.0|       0.0|     0.0|    0.0|[3.0,24.0,2.0,2.0...|  0.0|\n",
      "|          1.0|    1.0|    36.0|    2.0|    1.0| 6948.0|    0.0|       2.0|        2.0|       2.0|       0.0|              1.0|   2.0|35.0|       2.0|      0.0|    0.0|        3.0|       0.0|     1.0|    0.0|[1.0,36.0,2.0,1.0...|  0.0|\n",
      "|          1.0|    3.0|    12.0|    2.0|    3.0| 3059.0|    3.0|       3.0|        2.0|       0.0|       0.0|              3.0|   0.0|61.0|       2.0|      1.0|    0.0|        1.0|       0.0|     0.0|    0.0|[3.0,12.0,2.0,3.0...|  0.0|\n",
      "|          0.0|    1.0|    30.0|    4.0|    0.0| 5234.0|    0.0|       0.0|        4.0|       3.0|       0.0|              1.0|   2.0|28.0|       2.0|      1.0|    1.0|        3.0|       0.0|     0.0|    0.0|[1.0,30.0,4.0,0.0...|  1.0|\n",
      "|          0.0|    1.0|    12.0|    2.0|    0.0| 1295.0|    0.0|       1.0|        3.0|       1.0|       0.0|              0.0|   2.0|25.0|       2.0|      0.0|    0.0|        2.0|       0.0|     0.0|    0.0|(20,[0,1,2,4,6,7,...|  1.0|\n",
      "|          0.0|    0.0|    48.0|    2.0|    9.0| 4308.0|    0.0|       1.0|        3.0|       1.0|       0.0|              3.0|   1.0|24.0|       2.0|      0.0|    0.0|        2.0|       0.0|     0.0|    0.0|(20,[1,2,3,4,6,7,...|  1.0|\n",
      "|          1.0|    1.0|    12.0|    2.0|    3.0| 1567.0|    0.0|       2.0|        1.0|       1.0|       0.0|              0.0|   2.0|22.0|       2.0|      1.0|    0.0|        2.0|       0.0|     1.0|    0.0|[1.0,12.0,2.0,3.0...|  0.0|\n",
      "|          0.0|    0.0|    24.0|    4.0|    0.0| 1199.0|    0.0|       4.0|        4.0|       2.0|       0.0|              3.0|   2.0|60.0|       2.0|      1.0|    1.0|        1.0|       0.0|     0.0|    0.0|[0.0,24.0,4.0,0.0...|  1.0|\n",
      "|          1.0|    0.0|    15.0|    2.0|    0.0| 1403.0|    0.0|       2.0|        2.0|       1.0|       0.0|              3.0|   2.0|28.0|       2.0|      0.0|    0.0|        2.0|       0.0|     0.0|    0.0|(20,[1,2,4,6,7,8,...|  0.0|\n",
      "|          0.0|    0.0|    24.0|    2.0|    3.0| 1282.0|    1.0|       2.0|        4.0|       1.0|       0.0|              1.0|   2.0|32.0|       2.0|      1.0|    0.0|        1.0|       0.0|     0.0|    0.0|[0.0,24.0,2.0,3.0...|  1.0|\n",
      "|          1.0|    3.0|    24.0|    4.0|    3.0| 2424.0|    4.0|       4.0|        4.0|       2.0|       0.0|              3.0|   1.0|53.0|       2.0|      1.0|    1.0|        2.0|       0.0|     0.0|    0.0|[3.0,24.0,4.0,3.0...|  0.0|\n",
      "|          1.0|    0.0|    30.0|    0.0|    9.0| 8072.0|    4.0|       1.0|        2.0|       2.0|       0.0|              2.0|   2.0|25.0|       0.0|      1.0|    2.0|        2.0|       0.0|     0.0|    0.0|[0.0,30.0,0.0,9.0...|  0.0|\n",
      "|          0.0|    1.0|    24.0|    2.0|    1.0|12579.0|    0.0|       4.0|        4.0|       1.0|       0.0|              1.0|   3.0|44.0|       2.0|      2.0|    0.0|        3.0|       0.0|     1.0|    0.0|[1.0,24.0,2.0,1.0...|  1.0|\n",
      "|          1.0|    3.0|    24.0|    2.0|    3.0| 3430.0|    2.0|       4.0|        3.0|       2.0|       0.0|              1.0|   2.0|31.0|       2.0|      1.0|    0.0|        2.0|       1.0|     1.0|    0.0|[3.0,24.0,2.0,3.0...|  0.0|\n",
      "+-------------+-------+--------+-------+-------+-------+-------+----------+-----------+----------+----------+-----------------+------+----+----------+---------+-------+-----------+----------+--------+-------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#StringIndexer() una etiqueta indexer que mapea una columna de string de etiquetas a una columna ML de indices de \n",
    "#etiquetas. Si la columna de entrata es numerica, se convierte a string y se indexa el valor de cadena. Los indices\n",
    "#son en [0, numLabels], ordenados por la frecuencia de la etiqueta. Por lo que la etiqueta con mas frecuencia\n",
    "#recibe el index 0\n",
    "#Use a StringIndexer to retunr a DataFrame with the viability column added as a label\n",
    "labelIndexer = StringIndexer()\n",
    "labelIndexer.setInputCol(\"creditability\")\n",
    "labelIndexer.setOutputCol(\"label\")\n",
    "\n",
    "#fit().- Ajusta un modelo a el dataset de entrada con patametros opcionales\n",
    "#transform().- Tranforma los datos de entrada del dataset con parametros optionales. \n",
    "df3 = labelIndexer.fit(df2).transform(df2)\n",
    "#The transform method produced a new column: label.\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data Modeling\n",
    "#Use the randomSplit method to dived up df3 into trainingDF (70% of the input DataFrame) and \n",
    "#a testDataDF (20% of the input DataFrame), and for reproducibility use the deed 5043. The cache each \n",
    "#DataFrame in memory to maximize performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(creditability=0.0, balance=0.0, duration=6.0, history=2.0, purpose=6.0, amount=448.0, savings=0.0, employment=1.0, instPercent=4.0, sexMarried=1.0, guarantors=0.0, residenceDuration=3.0, assets=1.0, age=23.0, concCredit=2.0, apartment=1.0, credits=0.0, occupations=2.0, dependents=0.0, hasPhone=0.0, foreign=0.0, features=DenseVector([0.0, 6.0, 2.0, 6.0, 448.0, 0.0, 1.0, 4.0, 1.0, 0.0, 3.0, 1.0, 23.0, 2.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0]), label=1.0)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The data is split into a training data set and a test data set, 80% of the data is used to traing the model\n",
    "#20% will be used for testing\n",
    "splitSeed =5043\n",
    "#splitSeed =1800009193\n",
    "(split30DF, split70DF) = df3.randomSplit([.30,.70],splitSeed)\n",
    "testDataDF = split30DF.cache()\n",
    "trainingDF = split70DF.cache()\n",
    "trainingDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Params.explainParams of RandomForestClassifier_41d1af088b2c19b58f03>\n"
     ]
    }
   ],
   "source": [
    "#Inicializar el algoritmo de Random Forest.\n",
    "rd =RandomForestClassifier()\n",
    "print (rd.explainParams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RandomForestClassificationModel (uid=rfc_0d84bdc4d387) with 20 trees\\n  Tree 0 (weight 1.0):\\n    If (feature 0 <= 1.0)\\n     If (feature 12 <= 25.0)\\n      If (feature 3 <= 0.0)\\n       Predict: 1.0\\n      Else (feature 3 > 0.0)\\n       Predict: 1.0\\n     Else (feature 12 > 25.0)\\n      If (feature 16 <= 1.0)\\n       Predict: 0.0\\n      Else (feature 16 > 1.0)\\n       Predict: 0.0\\n    Else (feature 0 > 1.0)\\n     If (feature 4 <= 6403.0)\\n      If (feature 3 <= 4.0)\\n       Predict: 0.0\\n      Else (feature 3 > 4.0)\\n       Predict: 0.0\\n     Else (feature 4 > 6403.0)\\n      If (feature 7 <= 3.0)\\n       Predict: 0.0\\n      Else (feature 7 > 3.0)\\n       Predict: 1.0\\n  Tree 1 (weight 1.0):\\n    If (feature 2 <= 3.0)\\n     If (feature 15 <= 1.0)\\n      If (feature 2 <= 1.0)\\n       Predict: 0.0\\n      Else (feature 2 > 1.0)\\n       Predict: 0.0\\n     Else (feature 15 > 1.0)\\n      Predict: 1.0\\n    Else (feature 2 > 3.0)\\n     If (feature 12 <= 25.0)\\n      If (feature 1 <= 24.0)\\n       Predict: 0.0\\n      Else (feature 1 > 24.0)\\n       Predict: 1.0\\n     Else (feature 12 > 25.0)\\n      If (feature 5 <= 0.0)\\n       Predict: 0.0\\n      Else (feature 5 > 0.0)\\n       Predict: 0.0\\n  Tree 2 (weight 1.0):\\n    If (feature 14 <= 1.0)\\n     If (feature 13 <= 0.0)\\n      If (feature 3 <= 6.0)\\n       Predict: 1.0\\n      Else (feature 3 > 6.0)\\n       Predict: 0.0\\n     Else (feature 13 > 0.0)\\n      If (feature 1 <= 39.0)\\n       Predict: 0.0\\n      Else (feature 1 > 39.0)\\n       Predict: 1.0\\n    Else (feature 14 > 1.0)\\n     If (feature 5 <= 3.0)\\n      If (feature 1 <= 42.0)\\n       Predict: 0.0\\n      Else (feature 1 > 42.0)\\n       Predict: 1.0\\n     Else (feature 5 > 3.0)\\n      If (feature 12 <= 30.0)\\n       Predict: 0.0\\n      Else (feature 12 > 30.0)\\n       Predict: 0.0\\n  Tree 3 (weight 1.0):\\n    If (feature 6 <= 1.0)\\n     If (feature 12 <= 57.0)\\n      If (feature 5 <= 2.0)\\n       Predict: 1.0\\n      Else (feature 5 > 2.0)\\n       Predict: 0.0\\n     Else (feature 12 > 57.0)\\n      Predict: 0.0\\n    Else (feature 6 > 1.0)\\n     If (feature 3 <= 5.0)\\n      If (feature 5 <= 0.0)\\n       Predict: 0.0\\n      Else (feature 5 > 0.0)\\n       Predict: 0.0\\n     Else (feature 3 > 5.0)\\n      If (feature 0 <= 1.0)\\n       Predict: 1.0\\n      Else (feature 0 > 1.0)\\n       Predict: 0.0\\n  Tree 4 (weight 1.0):\\n    If (feature 0 <= 1.0)\\n     If (feature 3 <= 0.0)\\n      If (feature 12 <= 25.0)\\n       Predict: 1.0\\n      Else (feature 12 > 25.0)\\n       Predict: 0.0\\n     Else (feature 3 > 0.0)\\n      If (feature 11 <= 0.0)\\n       Predict: 0.0\\n      Else (feature 11 > 0.0)\\n       Predict: 0.0\\n    Else (feature 0 > 1.0)\\n     If (feature 6 <= 1.0)\\n      If (feature 16 <= 1.0)\\n       Predict: 0.0\\n      Else (feature 16 > 1.0)\\n       Predict: 0.0\\n     Else (feature 6 > 1.0)\\n      If (feature 12 <= 28.0)\\n       Predict: 0.0\\n      Else (feature 12 > 28.0)\\n       Predict: 0.0\\n  Tree 5 (weight 1.0):\\n    If (feature 1 <= 20.0)\\n     If (feature 12 <= 25.0)\\n      If (feature 10 <= 1.0)\\n       Predict: 0.0\\n      Else (feature 10 > 1.0)\\n       Predict: 0.0\\n     Else (feature 12 > 25.0)\\n      If (feature 5 <= 0.0)\\n       Predict: 0.0\\n      Else (feature 5 > 0.0)\\n       Predict: 0.0\\n    Else (feature 1 > 20.0)\\n     If (feature 13 <= 1.0)\\n      If (feature 8 <= 0.0)\\n       Predict: 1.0\\n      Else (feature 8 > 0.0)\\n       Predict: 0.0\\n     Else (feature 13 > 1.0)\\n      If (feature 5 <= 0.0)\\n       Predict: 1.0\\n      Else (feature 5 > 0.0)\\n       Predict: 0.0\\n  Tree 6 (weight 1.0):\\n    If (feature 5 <= 1.0)\\n     If (feature 1 <= 42.0)\\n      If (feature 9 <= 1.0)\\n       Predict: 0.0\\n      Else (feature 9 > 1.0)\\n       Predict: 0.0\\n     Else (feature 1 > 42.0)\\n      If (feature 5 <= 0.0)\\n       Predict: 1.0\\n      Else (feature 5 > 0.0)\\n       Predict: 1.0\\n    Else (feature 5 > 1.0)\\n     If (feature 18 <= 0.0)\\n      If (feature 10 <= 0.0)\\n       Predict: 0.0\\n      Else (feature 10 > 0.0)\\n       Predict: 0.0\\n     Else (feature 18 > 0.0)\\n      If (feature 10 <= 0.0)\\n       Predict: 1.0\\n      Else (feature 10 > 0.0)\\n       Predict: 0.0\\n  Tree 7 (weight 1.0):\\n    If (feature 3 <= 0.0)\\n     If (feature 5 <= 1.0)\\n      If (feature 2 <= 1.0)\\n       Predict: 1.0\\n      Else (feature 2 > 1.0)\\n       Predict: 0.0\\n     Else (feature 5 > 1.0)\\n      If (feature 12 <= 23.0)\\n       Predict: 1.0\\n      Else (feature 12 > 23.0)\\n       Predict: 0.0\\n    Else (feature 3 > 0.0)\\n     If (feature 4 <= 3343.0)\\n      If (feature 0 <= 2.0)\\n       Predict: 0.0\\n      Else (feature 0 > 2.0)\\n       Predict: 0.0\\n     Else (feature 4 > 3343.0)\\n      If (feature 3 <= 1.0)\\n       Predict: 0.0\\n      Else (feature 3 > 1.0)\\n       Predict: 0.0\\n  Tree 8 (weight 1.0):\\n    If (feature 13 <= 1.0)\\n     If (feature 14 <= 0.0)\\n      If (feature 18 <= 0.0)\\n       Predict: 1.0\\n      Else (feature 18 > 0.0)\\n       Predict: 1.0\\n     Else (feature 14 > 0.0)\\n      If (feature 3 <= 1.0)\\n       Predict: 1.0\\n      Else (feature 3 > 1.0)\\n       Predict: 0.0\\n    Else (feature 13 > 1.0)\\n     If (feature 0 <= 1.0)\\n      If (feature 1 <= 11.0)\\n       Predict: 0.0\\n      Else (feature 1 > 11.0)\\n       Predict: 0.0\\n     Else (feature 0 > 1.0)\\n      If (feature 2 <= 3.0)\\n       Predict: 0.0\\n      Else (feature 2 > 3.0)\\n       Predict: 0.0\\n  Tree 9 (weight 1.0):\\n    If (feature 6 <= 1.0)\\n     If (feature 10 <= 1.0)\\n      If (feature 3 <= 0.0)\\n       Predict: 1.0\\n      Else (feature 3 > 0.0)\\n       Predict: 0.0\\n     Else (feature 10 > 1.0)\\n      If (feature 1 <= 15.0)\\n       Predict: 0.0\\n      Else (feature 1 > 15.0)\\n       Predict: 1.0\\n    Else (feature 6 > 1.0)\\n     If (feature 5 <= 0.0)\\n      If (feature 0 <= 1.0)\\n       Predict: 1.0\\n      Else (feature 0 > 1.0)\\n       Predict: 0.0\\n     Else (feature 5 > 0.0)\\n      If (feature 9 <= 1.0)\\n       Predict: 0.0\\n      Else (feature 9 > 1.0)\\n       Predict: 0.0\\n  Tree 10 (weight 1.0):\\n    If (feature 12 <= 25.0)\\n     If (feature 5 <= 2.0)\\n      If (feature 1 <= 15.0)\\n       Predict: 0.0\\n      Else (feature 1 > 15.0)\\n       Predict: 1.0\\n     Else (feature 5 > 2.0)\\n      If (feature 1 <= 6.0)\\n       Predict: 1.0\\n      Else (feature 1 > 6.0)\\n       Predict: 0.0\\n    Else (feature 12 > 25.0)\\n     If (feature 4 <= 3343.0)\\n      If (feature 0 <= 2.0)\\n       Predict: 0.0\\n      Else (feature 0 > 2.0)\\n       Predict: 0.0\\n     Else (feature 4 > 3343.0)\\n      If (feature 16 <= 2.0)\\n       Predict: 0.0\\n      Else (feature 16 > 2.0)\\n       Predict: 1.0\\n  Tree 11 (weight 1.0):\\n    If (feature 4 <= 9857.0)\\n     If (feature 4 <= 3966.0)\\n      If (feature 0 <= 2.0)\\n       Predict: 0.0\\n      Else (feature 0 > 2.0)\\n       Predict: 0.0\\n     Else (feature 4 > 3966.0)\\n      If (feature 5 <= 0.0)\\n       Predict: 1.0\\n      Else (feature 5 > 0.0)\\n       Predict: 0.0\\n    Else (feature 4 > 9857.0)\\n     If (feature 6 <= 2.0)\\n      If (feature 7 <= 2.0)\\n       Predict: 1.0\\n      Else (feature 7 > 2.0)\\n       Predict: 1.0\\n     Else (feature 6 > 2.0)\\n      If (feature 2 <= 2.0)\\n       Predict: 1.0\\n      Else (feature 2 > 2.0)\\n       Predict: 0.0\\n  Tree 12 (weight 1.0):\\n    If (feature 13 <= 0.0)\\n     If (feature 5 <= 1.0)\\n      If (feature 15 <= 0.0)\\n       Predict: 1.0\\n      Else (feature 15 > 0.0)\\n       Predict: 1.0\\n     Else (feature 5 > 1.0)\\n      If (feature 7 <= 3.0)\\n       Predict: 0.0\\n      Else (feature 7 > 3.0)\\n       Predict: 0.0\\n    Else (feature 13 > 0.0)\\n     If (feature 16 <= 2.0)\\n      If (feature 0 <= 1.0)\\n       Predict: 0.0\\n      Else (feature 0 > 1.0)\\n       Predict: 0.0\\n     Else (feature 16 > 2.0)\\n      If (feature 1 <= 39.0)\\n       Predict: 0.0\\n      Else (feature 1 > 39.0)\\n       Predict: 1.0\\n  Tree 13 (weight 1.0):\\n    If (feature 5 <= 1.0)\\n     If (feature 13 <= 1.0)\\n      If (feature 3 <= 0.0)\\n       Predict: 1.0\\n      Else (feature 3 > 0.0)\\n       Predict: 0.0\\n     Else (feature 13 > 1.0)\\n      If (feature 1 <= 42.0)\\n       Predict: 0.0\\n      Else (feature 1 > 42.0)\\n       Predict: 1.0\\n    Else (feature 5 > 1.0)\\n     If (feature 3 <= 3.0)\\n      If (feature 0 <= 0.0)\\n       Predict: 0.0\\n      Else (feature 0 > 0.0)\\n       Predict: 0.0\\n     Else (feature 3 > 3.0)\\n      If (feature 16 <= 1.0)\\n       Predict: 0.0\\n      Else (feature 16 > 1.0)\\n       Predict: 0.0\\n  Tree 14 (weight 1.0):\\n    If (feature 14 <= 0.0)\\n     If (feature 3 <= 3.0)\\n      If (feature 8 <= 1.0)\\n       Predict: 0.0\\n      Else (feature 8 > 1.0)\\n       Predict: 0.0\\n     Else (feature 3 > 3.0)\\n      If (feature 1 <= 18.0)\\n       Predict: 0.0\\n      Else (feature 1 > 18.0)\\n       Predict: 1.0\\n    Else (feature 14 > 0.0)\\n     If (feature 6 <= 1.0)\\n      If (feature 9 <= 1.0)\\n       Predict: 0.0\\n      Else (feature 9 > 1.0)\\n       Predict: 0.0\\n     Else (feature 6 > 1.0)\\n      If (feature 1 <= 30.0)\\n       Predict: 0.0\\n      Else (feature 1 > 30.0)\\n       Predict: 0.0\\n  Tree 15 (weight 1.0):\\n    If (feature 0 <= 0.0)\\n     If (feature 1 <= 11.0)\\n      If (feature 2 <= 2.0)\\n       Predict: 0.0\\n      Else (feature 2 > 2.0)\\n       Predict: 0.0\\n     Else (feature 1 > 11.0)\\n      If (feature 16 <= 1.0)\\n       Predict: 0.0\\n      Else (feature 16 > 1.0)\\n       Predict: 1.0\\n    Else (feature 0 > 0.0)\\n     If (feature 10 <= 2.0)\\n      If (feature 7 <= 1.0)\\n       Predict: 0.0\\n      Else (feature 7 > 1.0)\\n       Predict: 0.0\\n     Else (feature 10 > 2.0)\\n      If (feature 16 <= 2.0)\\n       Predict: 0.0\\n      Else (feature 16 > 2.0)\\n       Predict: 0.0\\n  Tree 16 (weight 1.0):\\n    If (feature 1 <= 15.0)\\n     If (feature 2 <= 1.0)\\n      If (feature 19 <= 0.0)\\n       Predict: 1.0\\n      Else (feature 19 > 0.0)\\n       Predict: 0.0\\n     Else (feature 2 > 1.0)\\n      If (feature 12 <= 22.0)\\n       Predict: 0.0\\n      Else (feature 12 > 22.0)\\n       Predict: 0.0\\n    Else (feature 1 > 15.0)\\n     If (feature 5 <= 1.0)\\n      If (feature 6 <= 1.0)\\n       Predict: 1.0\\n      Else (feature 6 > 1.0)\\n       Predict: 0.0\\n     Else (feature 5 > 1.0)\\n      If (feature 8 <= 1.0)\\n       Predict: 0.0\\n      Else (feature 8 > 1.0)\\n       Predict: 0.0\\n  Tree 17 (weight 1.0):\\n    If (feature 6 <= 1.0)\\n     If (feature 0 <= 1.0)\\n      If (feature 9 <= 1.0)\\n       Predict: 1.0\\n      Else (feature 9 > 1.0)\\n       Predict: 0.0\\n     Else (feature 0 > 1.0)\\n      If (feature 1 <= 28.0)\\n       Predict: 0.0\\n      Else (feature 1 > 28.0)\\n       Predict: 1.0\\n    Else (feature 6 > 1.0)\\n     If (feature 1 <= 30.0)\\n      If (feature 4 <= 7865.0)\\n       Predict: 0.0\\n      Else (feature 4 > 7865.0)\\n       Predict: 1.0\\n     Else (feature 1 > 30.0)\\n      If (feature 14 <= 1.0)\\n       Predict: 0.0\\n      Else (feature 14 > 1.0)\\n       Predict: 1.0\\n  Tree 18 (weight 1.0):\\n    If (feature 4 <= 4380.0)\\n     If (feature 14 <= 1.0)\\n      If (feature 5 <= 1.0)\\n       Predict: 0.0\\n      Else (feature 5 > 1.0)\\n       Predict: 0.0\\n     Else (feature 14 > 1.0)\\n      If (feature 2 <= 3.0)\\n       Predict: 1.0\\n      Else (feature 2 > 3.0)\\n       Predict: 0.0\\n    Else (feature 4 > 4380.0)\\n     If (feature 8 <= 1.0)\\n      If (feature 0 <= 1.0)\\n       Predict: 1.0\\n      Else (feature 0 > 1.0)\\n       Predict: 0.0\\n     Else (feature 8 > 1.0)\\n      If (feature 12 <= 38.0)\\n       Predict: 0.0\\n      Else (feature 12 > 38.0)\\n       Predict: 1.0\\n  Tree 19 (weight 1.0):\\n    If (feature 1 <= 14.0)\\n     If (feature 0 <= 1.0)\\n      If (feature 6 <= 1.0)\\n       Predict: 0.0\\n      Else (feature 6 > 1.0)\\n       Predict: 0.0\\n     Else (feature 0 > 1.0)\\n      If (feature 12 <= 26.0)\\n       Predict: 0.0\\n      Else (feature 12 > 26.0)\\n       Predict: 0.0\\n    Else (feature 1 > 14.0)\\n     If (feature 5 <= 0.0)\\n      If (feature 7 <= 3.0)\\n       Predict: 0.0\\n      Else (feature 7 > 3.0)\\n       Predict: 1.0\\n     Else (feature 5 > 0.0)\\n      If (feature 8 <= 1.0)\\n       Predict: 0.0\\n      Else (feature 8 > 1.0)\\n       Predict: 0.0\\n'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create the classifier, set parameters for training\n",
    "#MaxDepth: Maximo profundida de un arbol. Aumentando la profundidad hace el modelo mas poderoso, \n",
    "#pero arboles profundos toma mas tiempo de entrenar.\n",
    "#MaxBins: Maximo numero de bins usados para discretizacion de caracteristicas continuas y\n",
    "#para escoger como dividir en caracteristicas en cada nodo. \n",
    "#Impurity: Criterio usado para que la informacion gane calculo.\n",
    "#auto: Automaticamente seleccionar el numero de caracteristicas a considerar para dividir en cada node del arbol\n",
    "#seed: Usa un numero de semilla aleatorio, permitiendo la repeticion de los resultados. \n",
    "classifier = (rd.setImpurity(\"gini\")\n",
    "              .setMaxDepth(3)\n",
    "              .setNumTrees(20)\n",
    "              .setFeatureSubsetStrategy(\"auto\")\n",
    "              .setSeed(5043)\n",
    "             )\n",
    "\n",
    "#use the random forest classifier to train (fit) the model\n",
    "model = classifier.fit(trainingDF)\n",
    "#print out the random forest trees\n",
    "model.toDebugString\n",
    "#model.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+--------+-------+-------+-------+-------+----------+-----------+----------+----------+-----------------+------+----+----------+---------+-------+-----------+----------+--------+-------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|creditability|balance|duration|history|purpose| amount|savings|employment|instPercent|sexMarried|guarantors|residenceDuration|assets| age|concCredit|apartment|credits|occupations|dependents|hasPhone|foreign|            features|label|       rawPrediction|         probability|prediction|\n",
      "+-------------+-------+--------+-------+-------+-------+-------+----------+-----------+----------+----------+-----------------+------+----+----------+---------+-------+-----------+----------+--------+-------+--------------------+-----+--------------------+--------------------+----------+\n",
      "|          0.0|    0.0|    12.0|    2.0|    1.0| 3386.0|    0.0|       4.0|        3.0|       2.0|       0.0|              3.0|   3.0|35.0|       2.0|      2.0|    0.0|        2.0|       0.0|     1.0|    0.0|[0.0,12.0,2.0,1.0...|  1.0|[13.2445364781399...|[0.66222682390699...|       0.0|\n",
      "|          0.0|    0.0|    12.0|    2.0|    2.0| 1282.0|    0.0|       2.0|        2.0|       1.0|       0.0|              3.0|   2.0|20.0|       2.0|      0.0|    0.0|        2.0|       0.0|     0.0|    0.0|(20,[1,2,3,4,6,7,...|  1.0|[12.9150495236675...|[0.64575247618337...|       0.0|\n",
      "|          0.0|    0.0|    15.0|    0.0|    0.0|  950.0|    0.0|       4.0|        4.0|       2.0|       0.0|              2.0|   2.0|33.0|       2.0|      0.0|    1.0|        2.0|       1.0|     0.0|    0.0|(20,[1,4,6,7,8,10...|  1.0|[11.5010275298146...|[0.57505137649073...|       0.0|\n",
      "|          0.0|    0.0|    18.0|    0.0|    2.0| 3114.0|    0.0|       1.0|        1.0|       1.0|       0.0|              3.0|   1.0|26.0|       2.0|      0.0|    0.0|        2.0|       0.0|     0.0|    0.0|(20,[1,3,4,6,7,8,...|  1.0|[11.6525412749411...|[0.58262706374705...|       0.0|\n",
      "|          0.0|    0.0|    18.0|    2.0|    2.0| 2473.0|    0.0|       0.0|        4.0|       2.0|       0.0|              0.0|   2.0|25.0|       2.0|      1.0|    0.0|        0.0|       0.0|     0.0|    0.0|(20,[1,2,3,4,7,8,...|  1.0|[11.4063728227355...|[0.57031864113677...|       0.0|\n",
      "|          0.0|    0.0|    21.0|    1.0|    0.0| 1647.0|    4.0|       2.0|        4.0|       2.0|       0.0|              1.0|   1.0|40.0|       2.0|      1.0|    1.0|        1.0|       1.0|     0.0|    0.0|[0.0,21.0,1.0,0.0...|  1.0|[15.0308356631039...|[0.75154178315519...|       0.0|\n",
      "|          0.0|    0.0|    21.0|    2.0|    3.0| 1835.0|    0.0|       2.0|        3.0|       1.0|       0.0|              1.0|   0.0|25.0|       2.0|      1.0|    1.0|        2.0|       0.0|     1.0|    0.0|[0.0,21.0,2.0,3.0...|  1.0|[12.4869898011006...|[0.62434949005503...|       0.0|\n",
      "|          0.0|    0.0|    24.0|    2.0|    0.0|  915.0|    4.0|       4.0|        4.0|       1.0|       0.0|              1.0|   2.0|29.0|       0.0|      1.0|    0.0|        2.0|       0.0|     0.0|    0.0|(20,[1,2,4,5,6,7,...|  1.0|[13.4632760718960...|[0.67316380359480...|       0.0|\n",
      "|          0.0|    0.0|    24.0|    2.0|    2.0| 2996.0|    4.0|       2.0|        2.0|       3.0|       0.0|              3.0|   2.0|20.0|       2.0|      1.0|    0.0|        2.0|       0.0|     0.0|    0.0|[0.0,24.0,2.0,2.0...|  1.0|[14.6040166068235...|[0.73020083034117...|       0.0|\n",
      "|          0.0|    0.0|    24.0|    2.0|    3.0| 1282.0|    1.0|       2.0|        4.0|       1.0|       0.0|              1.0|   2.0|32.0|       2.0|      1.0|    0.0|        1.0|       0.0|     0.0|    0.0|[0.0,24.0,2.0,3.0...|  1.0|[14.0481933515844...|[0.70240966757922...|       0.0|\n",
      "|          0.0|    0.0|    24.0|    3.0|    3.0| 1024.0|    0.0|       1.0|        4.0|       3.0|       0.0|              3.0|   0.0|48.0|       1.0|      1.0|    0.0|        2.0|       0.0|     0.0|    0.0|(20,[1,2,3,4,6,7,...|  1.0|[11.7952890508919...|[0.58976445254459...|       0.0|\n",
      "|          0.0|    0.0|    24.0|    3.0|    3.0| 1659.0|    0.0|       1.0|        4.0|       1.0|       0.0|              1.0|   2.0|29.0|       2.0|      0.0|    0.0|        1.0|       0.0|     1.0|    0.0|[0.0,24.0,3.0,3.0...|  1.0|[11.9156403043281...|[0.59578201521640...|       0.0|\n",
      "|          0.0|    0.0|    30.0|    2.0|    5.0|11998.0|    0.0|       1.0|        1.0|       0.0|       0.0|              0.0|   3.0|34.0|       2.0|      1.0|    0.0|        1.0|       0.0|     1.0|    0.0|(20,[1,2,3,4,6,7,...|  1.0|[11.0186207211704...|[0.55093103605852...|       0.0|\n",
      "|          0.0|    0.0|    36.0|    1.0|    2.0| 2746.0|    0.0|       4.0|        4.0|       2.0|       0.0|              3.0|   2.0|31.0|       0.0|      1.0|    0.0|        2.0|       0.0|     0.0|    0.0|(20,[1,2,3,4,6,7,...|  1.0|[11.6166609974286...|[0.58083304987143...|       0.0|\n",
      "|          0.0|    0.0|    36.0|    2.0|    6.0| 1977.0|    4.0|       4.0|        4.0|       2.0|       0.0|              3.0|   3.0|40.0|       2.0|      1.0|    0.0|        3.0|       0.0|     1.0|    0.0|[0.0,36.0,2.0,6.0...|  1.0|[13.8037173330489...|[0.69018586665244...|       0.0|\n",
      "|          0.0|    0.0|    48.0|    0.0|    1.0| 4605.0|    0.0|       4.0|        3.0|       2.0|       0.0|              3.0|   3.0|24.0|       2.0|      2.0|    1.0|        2.0|       1.0|     0.0|    0.0|[0.0,48.0,0.0,1.0...|  1.0|[9.69954528171734...|[0.48497726408586...|       1.0|\n",
      "|          0.0|    1.0|     6.0|    3.0|    0.0| 1209.0|    0.0|       0.0|        4.0|       2.0|       0.0|              3.0|   1.0|47.0|       2.0|      1.0|    0.0|        3.0|       0.0|     1.0|    0.0|[1.0,6.0,3.0,0.0,...|  1.0|[13.2803067159728...|[0.66401533579864...|       0.0|\n",
      "|          0.0|    1.0|     8.0|    2.0|    2.0| 1237.0|    0.0|       2.0|        3.0|       1.0|       0.0|              3.0|   0.0|24.0|       2.0|      1.0|    0.0|        2.0|       0.0|     0.0|    0.0|[1.0,8.0,2.0,2.0,...|  1.0|[14.4878575662910...|[0.72439287831455...|       0.0|\n",
      "|          0.0|    1.0|     9.0|    4.0|    6.0| 1501.0|    0.0|       4.0|        2.0|       1.0|       0.0|              2.0|   2.0|34.0|       2.0|      1.0|    1.0|        3.0|       0.0|     1.0|    0.0|[1.0,9.0,4.0,6.0,...|  1.0|[14.0551993291787...|[0.70275996645893...|       0.0|\n",
      "|          0.0|    1.0|    12.0|    2.0|    0.0| 1223.0|    0.0|       4.0|        1.0|       0.0|       0.0|              0.0|   0.0|46.0|       2.0|      0.0|    1.0|        2.0|       0.0|     0.0|    0.0|(20,[0,1,2,4,6,7,...|  1.0|[13.6348094389001...|[0.68174047194500...|       0.0|\n",
      "+-------------+-------+--------+-------+-------+-------+-------+----------+-----------+----------+----------+-----------------+------+----+----------+---------+-------+-----------+----------+--------+-------+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Use the test data to get predictions\n",
    "#run the model on test features to get predictions\n",
    "predictions = model.transform(testDataDF)\n",
    "#predictions.take(1)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7735015437027579\n"
     ]
    }
   ],
   "source": [
    "#Below we evaluate the predictions, we use a BinaryClassificationEvaluator which returns a \n",
    "#precision metric (The Area Under an ROC Curve) by comparing the test label column with the test \n",
    "#prediction column. In this case the evaluation #returns 50% precision.\n",
    "# create an Evaluator for binary classification, which expects two input columns: rawPrediction and label.\n",
    "evaluator = BinaryClassificationEvaluator().setRawPredictionCol(\"rawPrediction\").setLabelCol(\"label\")\n",
    "# Evaluates predictions and returns a scalar metric areaUnderROC(larger is better).\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#From a visual inspection of the predictions, we can see that they are close to the actual values.\n",
    "\n",
    "#However, we would like a scientific measure of how well the Random Forest model is performing in \n",
    "#accurately predicting values. To perform this measurement, we can use an evaluation metric such \n",
    "#as [Root Mean Squared Error](https://en.wikipedia.org/wiki/Root-mean-square_deviation) (RMSE) to \n",
    "#validate our Random Forest model.\n",
    "\n",
    "#RSME is defined as follows: \\\\( RMSE = \\sqrt{\\frac{\\sum_{i = 1}^{n} (x_i - y_i)^2}{n}}\\\\) where \\\\(y_i\\\\) \n",
    "#is the observed value and \\\\(x_i\\\\) is the predicted value\n",
    "\n",
    "#RMSE is a frequently used measure of the differences between values predicted by a model or an \n",
    "#estimator and the values actually observed. The lower the RMSE, the better our model.\n",
    "\n",
    "#Spark ML Pipeline provides several regression analysis metrics, including [RegressionEvaluator()]\n",
    "#(https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator).\n",
    "\n",
    "#After we create an instance of [RegressionEvaluator]\n",
    "#(https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator), \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error: 0.54\n",
      "Mean Absolute Error: 0.29\n"
     ]
    }
   ],
   "source": [
    "# Now let's compute an evaluation metric for our test dataset\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create an RMSE evaluator using the label and predicted columns\n",
    "regEval = RegressionEvaluator(predictionCol = \"prediction\", labelCol = \"label\", metricName = \"rmse\")\n",
    "#Create an MAE evaluator using the label and predicted columns\n",
    "regEvals = RegressionEvaluator(predictionCol = \"prediction\", labelCol = \"label\", metricName = \"mae\")\n",
    "\n",
    "# Run the evaluator on the DataFrame\n",
    "rmse = regEval.evaluate(predictions)\n",
    "mae = regEvals.evaluate(predictions)\n",
    "\n",
    "print (\"Root Mean Squared Error: %.2f\" % rmse)\n",
    "print (\"Mean Absolute Error: %.2f\" % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tuning and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Crea y configura un pipeline. \n",
    "#Pipeline:- El cual actua como un estimador. Un pipeline consiste de secuencias de estados,\n",
    "#el cual cada uno es un estimador o un transformador. \n",
    "steps = [classifier]\n",
    "#Configura los estados del pipeline.\n",
    "pipeline = Pipeline().setStages(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Se uso la clase CrossValidator para la seleccion del modelo. El CrossValidator usa un Estimador,\n",
    "#un conjunto de ParamMaps, y un Evaluador. Note: Usar un CroosValidatos puede ser muy caro. \n",
    "#Evalua el modelo sobre instances de prueba y computa el error de prueba. \n",
    "evaluator = (BinaryClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "             )\n",
    "cv = (CrossValidator()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(evaluator)\n",
    "  .setNumFolds(10)\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We will next train the model using a pipeline, which can give better results. \n",
    "#A pipeline provides a simple way to try out different combinations of \n",
    "#parameters, using a process called grid search, where you set up the parameters \n",
    "#to test, and MLLib #will test all the combinations. Pipelines make it \n",
    "#easy to tune an entire model building workflow at once, rather than tuning each \n",
    "#element in the Pipeline separately.\n",
    "#regParam = [x / 100.0 for x in range(1,11)]\n",
    "#print (regParam)\n",
    "#Let's tune over our rf.maxBins parameter on the values 50 and 100,We use a ParamGridBuilder \n",
    "#to construct a grid of parameters to search over\n",
    "paramGrid = (ParamGridBuilder()\n",
    "  .addGrid(classifier.maxBins, [28,50,100])\n",
    "  .addGrid(classifier.maxDepth, [4, 6, 8])\n",
    "  .addGrid(classifier.impurity, (\"entropy\", \"gini\"))\n",
    "  .build()\n",
    "             )\n",
    "#Add the paramGrid to the CrossValidator\n",
    "cv.setEstimatorParamMaps(paramGrid)\n",
    "\n",
    "#Now let's find and return the best model\n",
    "cvModel = cv.fit(trainingDF).bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7905887362929842\n"
     ]
    }
   ],
   "source": [
    "#The pipeline automatically optimizes by exploring the parameter grid: for each \n",
    "#ParamMap, the CrossValidator trains the given Estimator and evaluates it using \n",
    "#the given Evaluator, then it fits the best Estimator using the best ParamMap \n",
    "#and the entire dataset.\n",
    "\n",
    "#When fit is called, the stages are executed in order.\n",
    "#Fit will run cross-validation,  and choose the best set of parameters.\n",
    "#The fitted model from a Pipeline is an PipelineModel, which consists of \n",
    "#fitted models and transformers\n",
    "pipelineFittedModel = cv.fit(trainingDF).bestModel\n",
    "#Now we can evaluate the pipeline best-fitted model by comparing test predictions \n",
    "#with test labels. \n",
    "#The evaluator now returns 50% accuracy compared to 50% before.\n",
    "#Call tranform to make predictions on test data. \n",
    "#The fitted model will use the best model found\n",
    "predictionsNew = pipelineFittedModel.transform(testDataDF)\n",
    "accuracyNew = evaluator.evaluate(predictionsNew)  \n",
    "print (accuracyNew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Acurracy: 77.35%\n",
      "New Acurracy: 79.06%\n",
      "Original Root Mean Squared Error: 0.54\n",
      "New Root Mean Squared Error: 0.53\n",
      "Mean Absolute Error: 0.29\n",
      "New Mean Absolute Error: 0.28\n"
     ]
    }
   ],
   "source": [
    "#Now let's use cvModel to compute an evaluation metric for our test dataset\n",
    "predictionAndLabelsDF = cvModel.transform(testDataDF)\n",
    "\n",
    "#Run the previously created RMSE evaluator, regEvalu, on true Predictions\n",
    "#And LabalsDF dataFrame\n",
    "rmseNew = regEval.evaluate(predictionAndLabelsDF,{regEval.metricName: \"rmse\"})\n",
    "maeNew = regEval.evaluate(predictionAndLabelsDF,{regEval.metricName: \"mae\"})\n",
    "\n",
    "print(\"Original Acurracy: {0:2.2f}%\".format(accuracy*100))\n",
    "print(\"New Acurracy: {0:2.2f}%\".format(accuracyNew*100))\n",
    "print(\"Original Root Mean Squared Error: {0:2.2f}\".format(rmse))\n",
    "print(\"New Root Mean Squared Error: {0:2.2f}\".format(rmseNew))\n",
    "print(\"Mean Absolute Error: {0:2.2f}\".format(mae))\n",
    "print(\"New Mean Absolute Error: {0:2.2f}\".format(maeNew))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
